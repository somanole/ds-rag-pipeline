{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e65b5289",
   "metadata": {},
   "source": [
    "## **Learning Objectives**\n",
    "\n",
    "By completing these exercises, you will:\n",
    "\n",
    "- Understand Retrieval-Augmented Generation (RAG) and its components.\n",
    "- Load, preprocess, and handle PDF documents effectively.\n",
    "- Convert textual data into embeddings for efficient retrieval.\n",
    "- Implement and test document retrieval systems using LangChain and FAISS.\n",
    "- Integrate retrieval systems with free Language Models (LLMs) from ChatGroq .\n",
    "- Build an interactive chat-based Q&A system.\n",
    "\n",
    "---\n",
    "\n",
    "## **Exercise 1: Setup and Warm-up**\n",
    "\n",
    "In this exercise, you'll set up your environment and select a suitable language model.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. **Load Environment Variables:** Ensure your environment variables (e.g., API keys, tokens) are securely stored and loaded.\n",
    "2. **Choose LLM:** Select a free LLM model from from ChatGroq. \n",
    "3. **Instantiate the Model:** Create an instance of your chosen model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c726e326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from dotenv import load_dotenv\n",
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "import warnings\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model=\"llama-3.1-8b-instant\", #\"llama3-8b-8192\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855d3984",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Exercise 2: Data Ingestion**\n",
    "\n",
    "In this exercise, you'll learn to load PDF data into a Python environment.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. **Import PDF Loader:** Use LangChain’s `PyPDFLoader`.\n",
    "2. **Load PDF File:** Create a function to read the PDF file.\n",
    "3. **Display PDF Content:** Print the number of pages and first page content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8e52390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PyPDFLoader\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# Example function to load PDF\n",
    "\n",
    "def load_pdf(pdf_path):\n",
    "    \"\"\"Load a PDF and return a list of LangChain Document objects.\"\"\"\n",
    "    loader = PyPDFLoader(file_path=pdf_path)\n",
    "    documents = loader.load()\n",
    "    return documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a740306b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pages loaded: 33\n",
      "\n",
      "First page preview:\n",
      "\n",
      "Published as a conference paper at ICLR 2023\n",
      "REAC T: S YNERGIZING REASONING AND ACTING IN\n",
      "LANGUAGE MODELS\n",
      "Shunyu Yao∗*,1, Jeffrey Zhao2, Dian Yu2, Nan Du2, Izhak Shafran2, Karthik Narasimhan1, Yuan Cao2\n",
      "1Department of Computer Science, Princeton University\n",
      "2Google Research, Brain team\n",
      "1{shunyuy,karthikn}@princeton.edu\n",
      "2{jeffreyzhao,dianyu,dunan,izhak,yuancao}@google.com\n",
      "ABSTRACT\n",
      "While large language models (LLMs) have demonstrated impressive performance\n",
      "across tasks in language understanding and interactive decision making, their\n",
      "abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action\n",
      "plan generation) have primarily been studied as separate topics. In this paper, we\n",
      "explore the use of LLMs to generate both reasoning traces and task-speciﬁc actions\n",
      "in an interleaved manner, allowing for greater synergy between the two: reasoning\n",
      "traces help the model induce, track, and update action plans as well as handle\n",
      "exceptions, while actions allow it to interface with and gather additional information\n",
      "from external sources such as knowledge bases or environments. We apply our\n",
      "approach, named ReAct, to a diverse set of language and decision making tasks\n",
      "and demonstrat\n"
     ]
    }
   ],
   "source": [
    "# Load your PDF and print out content here\n",
    "react_docs = load_pdf(\"../documents/react_paper.pdf\")\n",
    "\n",
    "print(f\"Number of pages loaded: {len(react_docs)}\")\n",
    "print(\"\\nFirst page preview:\\n\")\n",
    "print(react_docs[0].page_content[:1200])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e85a2ec",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Exercise 3: Document Chunking**\n",
    "\n",
    "This exercise introduces splitting large documents into manageable text chunks.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. **Import Text Splitter:** Use `RecursiveCharacterTextSplitter`.\n",
    "2. **Chunk Document:** Write a function that splits loaded documents into chunks.\n",
    "3. **Test Function:** Verify by displaying the resulting chunks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d497c03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import RecursiveCharacterTextSplitter\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Example chunking function\n",
    "def chunk_documents(documents, chunk_size=150, chunk_overlap=40):\n",
    "    \"\"\"Split loaded documents into overlapping chunks.\"\"\"\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "    )\n",
    "    chunks = splitter.split_documents(documents)\n",
    "\n",
    "    # Add a stable chunk id for easier debugging/inspection.\n",
    "    for idx, chunk in enumerate(chunks):\n",
    "        chunk.metadata[\"id\"] = f\"chunk_{idx}\"\n",
    "\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4265a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks created: 1094\n",
      "\n",
      "First chunk metadata:\n",
      " {'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-13T00:09:11+00:00', 'author': '', 'keywords': '', 'moddate': '2023-03-13T00:09:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../documents/react_paper.pdf', 'total_pages': 33, 'page': 0, 'page_label': '1', 'id': 'chunk_0'}\n",
      "\n",
      "First chunk content:\n",
      " Published as a conference paper at ICLR 2023\n",
      "REAC T: S YNERGIZING REASONING AND ACTING IN\n",
      "LANGUAGE MODELS\n"
     ]
    }
   ],
   "source": [
    "# Execute your chunking function and display results here\n",
    "react_chunks = chunk_documents(react_docs, chunk_size=150, chunk_overlap=40)\n",
    "\n",
    "print(f\"Number of chunks created: {len(react_chunks)}\")\n",
    "print(\"\\nFirst chunk metadata:\\n\", react_chunks[0].metadata)\n",
    "print(\"\\nFirst chunk content:\\n\", react_chunks[0].page_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a61a64",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## **Exercise 4: Embedding and Storage**\n",
    "\n",
    "In this exercise, you will create embeddings from text chunks and store them efficiently.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. **Choose Embedding Model:** Use `sentence-transformers/all-mpnet-base-v2` from Hugging Face.\n",
    "2. **Generate Embeddings:** Transform document chunks into embeddings.\n",
    "3. **Store Embeddings:** Save these embeddings using FAISS locally.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d59e07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.vectorstores.faiss import DistanceStrategy\n",
    "\n",
    "# Example function for embeddings and storage\n",
    "def embed_and_store(chunks, db_name=\"react_exercise\"):\n",
    "    \"\"\"Create embeddings for chunks and store them in a local FAISS index.\"\"\"\n",
    "    embedding_model = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-mpnet-base-v2\",\n",
    "        encode_kwargs={\"normalize_embeddings\": True},\n",
    "    )\n",
    "\n",
    "    vectorstore = FAISS.from_documents(\n",
    "        documents=chunks,\n",
    "        embedding=embedding_model,\n",
    "        distance_strategy=DistanceStrategy.COSINE,\n",
    "    )\n",
    "\n",
    "    save_path = f\"../vector_databases/vector_db_{db_name}\"\n",
    "    vectorstore.save_local(save_path)\n",
    "    return vectorstore, save_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08151ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored FAISS index at: ../vector_databases/vector_db_react_exercise\n",
      "Documents in vector store: 1094\n"
     ]
    }
   ],
   "source": [
    "# Generate embeddings and save them locally\n",
    "react_vectorstore, react_vector_db_path = embed_and_store(react_chunks, db_name=\"react_exercise\")\n",
    "\n",
    "print(f\"Stored FAISS index at: {react_vector_db_path}\")\n",
    "print(f\"Documents in vector store: {react_vectorstore.index.ntotal}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ce9bba",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Exercise 5: Retrieval from FAISS**\n",
    "\n",
    "Here, you will learn how to retrieve documents from a vector database using embeddings.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. **Load Embeddings:** Load stored embeddings from the FAISS database.\n",
    "2. **Implement Retrieval:** Create logic to retrieve relevant chunks based on queries.\n",
    "3. **Test Retriever:** Execute retrieval using sample queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6775f587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement retrieval logic from your FAISS database\n",
    "def load_retriever(vector_db_path):\n",
    "    \"\"\"Load a local FAISS index and expose a retriever.\"\"\"\n",
    "    embedding_model = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-mpnet-base-v2\",\n",
    "        encode_kwargs={\"normalize_embeddings\": True},\n",
    "    )\n",
    "\n",
    "    vectorstore = FAISS.load_local(\n",
    "        folder_path=vector_db_path,\n",
    "        embeddings=embedding_model,\n",
    "        allow_dangerous_deserialization=True,\n",
    "        distance_strategy=DistanceStrategy.COSINE,\n",
    "    )\n",
    "\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "    return retriever, vectorstore\n",
    "\n",
    "\n",
    "react_retriever, react_vectorstore = load_retriever(react_vector_db_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ca40046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 3 documents for query: What is ReAct in language models?\n",
      "\n",
      "Result 1 metadata: {'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-13T00:09:11+00:00', 'author': '', 'keywords': '', 'moddate': '2023-03-13T00:09:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../documents/react_paper.pdf', 'total_pages': 33, 'page': 3, 'page_label': '4', 'id': 'chunk_148'}\n",
      "Since decision making and reasoning capabilities are integrated into a large language model, ReAct\n",
      "--------------------------------------------------------------------------------\n",
      "Result 2 metadata: {'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-13T00:09:11+00:00', 'author': '', 'keywords': '', 'moddate': '2023-03-13T00:09:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../documents/react_paper.pdf', 'total_pages': 33, 'page': 13, 'page_label': '14', 'id': 'chunk_545'}\n",
      "across different large language models on different tasks. The code for these experiments are at\n",
      "https://react-lm.github.io/.\n",
      "--------------------------------------------------------------------------------\n",
      "Result 3 metadata: {'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-13T00:09:11+00:00', 'author': '', 'keywords': '', 'moddate': '2023-03-13T00:09:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../documents/react_paper.pdf', 'total_pages': 33, 'page': 0, 'page_label': '1', 'id': 'chunk_13'}\n",
      "approach, named ReAct, to a diverse set of language and decision making tasks\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test your retrieval system with queries\n",
    "query = \"What is ReAct in language models?\"\n",
    "retrieved_docs = react_retriever.invoke(query)\n",
    "\n",
    "print(f\"Retrieved {len(retrieved_docs)} documents for query: {query}\\n\")\n",
    "for i, doc in enumerate(retrieved_docs, start=1):\n",
    "    print(f\"Result {i} metadata: {doc.metadata}\")\n",
    "    print(doc.page_content[:300])\n",
    "    print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdc5069",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Exercise 6: Connecting Retrieval with LLM**\n",
    "\n",
    "You'll now connect document retrieval with the Language Model.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. **Create Retrieval Chain:** Link your retrieval system to your instantiated LLM.\n",
    "2. **Test the Chain:** Confirm it works by generating answers from retrieved documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b1d6376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function to create retrieval and document processing chains\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "\n",
    "\n",
    "def create_rag_chain(retriever, llm):\n",
    "    \"\"\"Connect retriever with an LLM using a retrieval QA prompt.\"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"You are a helpful assistant. Use only the provided context to answer.\n",
    "If the answer is not in the context, say you do not know.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {input}\"\"\"\n",
    "    )\n",
    "\n",
    "    combine_docs_chain = create_stuff_documents_chain(llm=llm, prompt=prompt)\n",
    "    retrieval_chain = create_retrieval_chain(\n",
    "        retriever=retriever,\n",
    "        combine_docs_chain=combine_docs_chain,\n",
    "    )\n",
    "    return retrieval_chain\n",
    "\n",
    "\n",
    "react_retrieval_chain = create_rag_chain(react_retriever, llm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d200b6f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Give a short summary of the ReAct paper.\n",
      "Answer: Based on the provided context, I can summarize the ReAct paper as follows:\n",
      "\n",
      "The ReAct paper (arXiv:2210.03629v3) discusses a model called ReAct, which can change its behavior drastically when given certain hints or modifications, such as adding 17 to a specific point (Act 23). The paper aims to demonstrate the differences between ReAct and another model called IM, highlighting the importance of internal modifications in ReAct.\n"
     ]
    }
   ],
   "source": [
    "# Invoke your chain with a sample question\n",
    "sample_question = \"Give a short summary of the ReAct paper.\"\n",
    "response = react_retrieval_chain.invoke({\"input\": sample_question})\n",
    "\n",
    "print(\"Question:\", sample_question)\n",
    "print(\"Answer:\", response[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b052fd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Exercise 7: Interactive Chat System**\n",
    "\n",
    "In the final exercise, build an interactive chat-based query system.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. **Create Chat Interface:** Develop a simple function for interactive querying.\n",
    "2. **Run the Chat:** Allow users to ask questions and receive immediate responses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d0c0506c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your interactive chat querying function\n",
    "def interactive_chat(retrieval_chain):\n",
    "    \"\"\"Simple terminal-like chat loop for querying the indexed documents.\"\"\"\n",
    "    print(\"RAG chat is ready. Type 'exit' to stop.\")\n",
    "\n",
    "    while True:\n",
    "        user_question = input(\"\\nYou: \").strip()\n",
    "\n",
    "        if user_question.lower() in {\"exit\", \"quit\", \"q\"}:\n",
    "            print(\"Assistant: Session ended.\")\n",
    "            break\n",
    "\n",
    "        if not user_question:\n",
    "            print(\"Assistant: Please enter a question.\")\n",
    "            continue\n",
    "\n",
    "        result = retrieval_chain.invoke({\"input\": user_question})\n",
    "        print(\"Assistant:\", result[\"answer\"].strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638c56d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know. The provided context does not mention the specific problem that ReAct tries to solve. It only mentions the differences between ReAct and IM, and the ability of ReAct to change its behavior.\n"
     ]
    }
   ],
   "source": [
    "# Run and test your interactive chat system\n",
    "interactive_chat(react_retrieval_chain)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb30ce1f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Conclusion & Reflection**\n",
    "\n",
    "After completing these exercises:\n",
    "\n",
    "- Summarize key concepts learned.\n",
    "- Reflect on the effectiveness and limitations of the free LLM and RAG system you've built.\n",
    "- Consider how you might improve or extend your system in practical applications.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
