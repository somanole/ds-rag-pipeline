{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# <span style=\"color:green;\">RAG with Langchain and Llama</span>\n",
    "<hr style=\"border:2px solid black\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <span style=\"color:orange;\">Learning Objectives</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid black\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "By the end of this lesson, students will:\n",
    "\n",
    "- Understand Retrieval-Augmented Generation (RAG) and its applications.\n",
    "\n",
    "- Learn how to store, retrieve embeddings using FAISS.\n",
    "\n",
    "- Implement retrievers and chains for querying document databases.\n",
    "\n",
    "- Develop an interactive chat system for PDF documents using LangChain.\n",
    "\n",
    "- Learn best practices for embedding models and retrieval systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <span style=\"color:orange;\">1. Introduction to [RAG](https://weaviate.io/blog/introduction-to-rag#:~:text=RAG%20is%20a%20multi%2Dstep,prompt%2C%20and%20generates%20a%20response.) (Retrieval-Augmented Generation)</span>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Limitations of LLMs :**\n",
    "- Know nothing outside training data (e.g., up-to-date information, classified/private data).\n",
    "- Not specialized in specific use cases.\n",
    "- Tend to hallucinate confidently, possibly leading to misinformation.\n",
    "- Produce black box output: do not clarify what has led to the generation of particular content.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**What is RAG?**\n",
    "- Combines retrieval-based and generative AI techniques.\n",
    "- Enhances LLMs with external knowledge retrieval to improve accuracy and reduce hallucinations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fine-Tuning vs. RAG :**\n",
    "\n",
    "***Fine-Tuning***\n",
    "- Enhances model performance for specific use cases via Transfer Learning.\n",
    "- Changes model parameters, enhancing speed and reducing cost for specific tasks.\n",
    "- Useful for static datasets (e.g., specialized industry terminology).\n",
    "- Limitations: Cannot provide up-to-date information.\n",
    "\n",
    "\n",
    "***Retrieval Augmented Generation (RAG)***\n",
    "- Increases model capabilities through:\n",
    "  - **Retrieving** external, up-to-date information.\n",
    "  - **Augmenting** the original prompt with retrieved data.\n",
    "  - **Generating** a response based on both the prompt and retrieved information.\n",
    "- No need for Transfer Learning (LLM parameters remain unchanged).\n",
    "- Provides a white box output (transparency, fewer hallucinations).\n",
    "- Ideal for real-time, dynamic knowledge retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Key Components of RAG :**\n",
    "- **Embedding Models:** Convert text into numerical vectors for similarity search.\n",
    "- **VectorStore and Vector Databases** (FAISS, ChromaDB, Pinecone, Weaviate).\n",
    "- **Retriever:** Fetches relevant documents.\n",
    "- **LLM (Generator):** Generates responses using retrieved documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid black\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <span style=\"color:orange;\">2. Use Cases of RAG</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- <span style=\"color:orange;\">Enterprise Document Search (Retrieving company policies, research papers).</span>\n",
    "- Chatbots with Domain-Specific Knowledge (Customer support, legal, medical).\n",
    "- Coding Assistants (Fetching relevant code snippets from documentation).\n",
    "- Financial Reports Analysis (Summarizing earnings reports, news articles).\n",
    "- E-Learning and Research Assistants.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid black\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <span style=\"color:orange;\">3. Warm Up </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import warnings\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.prompts.prompt import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the LLM (Using Groq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model=\"llama-3.1-8b-instant\", #\"llama3-8b-8192\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define promt template\n",
    "\n",
    "**What is a Prompt?**\n",
    ">- A set of instructions or input for an LLM provided by a user to guide its response\n",
    ">- Helps the model understand context and generate relevant content and coherent language-based output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    given the information {information} about a person I want you to create:\n",
    "    1. A short summary\n",
    "    2. two interesting facts about them\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"information\"],\n",
    "    template=query\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Chain\n",
    "\n",
    "**What is a Chain?**\n",
    "\n",
    "> - Allows to link the output of one LLM call as the input of another"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt_template | llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:**\n",
    "- The `|` symbol chains together the different components, feeding the output from one component as input into the next component.\n",
    "- In this chain the user input is passed to the prompt template, then the prompt template output is passed to the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### invoke Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data =\"\"\"\n",
    "Geoffrey Everest Hinton (born 6 December 1947) is a British-Canadian computer scientist, cognitive scientist, \n",
    "cognitive psychologist, known for his work on artificial neural networks which earned him the title as the \n",
    "\"Godfather of AI\". Hinton is University Professor Emeritus at the University of Toronto. From 2013 to 2023, \n",
    "he divided his time working for Google (Google Brain) and the University of Toronto, before publicly announcing \n",
    "his departure from Google in May 2023, citing concerns about the risks of artificial intelligence (AI) technology.\n",
    "In 2017, he co-founded and became the chief scientific advisor of the Vector Institute in Toronto.\n",
    "\n",
    "With David Rumelhart and Ronald J. Williams, Hinton was co-author of a highly cited paper published in 1986 \n",
    "that popularised the backpropagation algorithm for training multi-layer neural networks, although they were \n",
    "not the first to propose the approach. Hinton is viewed as a leading figure in the deep learning community.\n",
    "The image-recognition milestone of the AlexNet designed in collaboration with his students Alex Krizhevsky \n",
    "and Ilya Sutskever for the ImageNet challenge 2012[22] was a breakthrough in the field of computer vision.\n",
    "\n",
    "Hinton received the 2018 Turing Award, often referred to as the \"Nobel Prize of Computing\", together with \n",
    "Yoshua Bengio and Yann LeCun, for their work on deep learning. They are sometimes referred to as the \n",
    "\"Godfathers of Deep Learning\", and have continued to give public talks together. He was also awarded \n",
    "the 2024 Nobel Prize in Physics, shared with John Hopfield.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = chain.invoke(input={\"information\": text_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid black\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <span style=\"color:orange;\">4. Implementing RAG with LangChain & Llama</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "This project utilizes **Retrieval-Augmented Generation (RAG)** to enhance the search and retrieval of **medical research papers**. By integrating **FAISS** and **LangChain**, we develop an intelligent system that efficiently retrieves relevant documents from a **VectorStore** and uses a **language model** to generate insightful responses based on those retrieved papers.  \n",
    "\n",
    "#### Objective  \n",
    "\n",
    "- Build a **RAG-based** system for retrieving **research papers**.  \n",
    "- Understand **embeddings, similarity search, and document retrieval** techniques.  \n",
    "- Implement efficient **document storage, search** using FAISS and LangChain.  \n",
    "\n",
    "#### Key Concepts  \n",
    "\n",
    "- **Embeddings:** Transforming text into numerical vectors for efficient retrieval.  \n",
    "- **VectorStores:** Storing and retrieving react research papers using embeddings.  \n",
    "- **Similarity Search:** Identifying the most relevant papers based on a given query.  \n",
    "- **LLM Integration:** Using a **Language Model** to enhance search results and generate meaningful responses from retrieved research papers.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"../Images/RAG_steps.png\" width=\"950\"/> \n",
    "\n",
    "üîó [**RAG Architecture**](https://weaviate.io/blog/introduction-to-rag#:~:text=RAG%20is%20a%20multi%2Dstep,prompt%2C%20and%20generates%20a%20response)\n",
    "\n",
    "\n",
    "#### Stages & Steps :\n",
    "\n",
    "üü£ Ingestion Stage:\n",
    "\n",
    "1. *Load PDF Data:*\n",
    "    - Use `PyPDFLoader` from LangChain to load and read PDF files.\n",
    "\n",
    "2. *Document Chunking:*\n",
    "    - Use `RecursiveCharacterTextSplitter` from LangChain to split documents into smaller chunks.\n",
    "\n",
    "3. *Embedding Storage:*\n",
    "    - Use `HuggingFaceEmbeddings` and `FAISS` from LangChain to convert chunks into vector embeddings and store them locally in a FAISS database.\n",
    "\n",
    "\n",
    "üü£ Inference Stage:\n",
    "\n",
    "4. *Retrieval Object Creation:*\n",
    "    - Use FAISS to load embedded chunks from the stored VectorStore and create a retrieval object.\n",
    "\n",
    "5. *Response Generation (Augmentation & Generation):*\n",
    "    - Use `create_stuff_documents_chain` and `create_retrieval_chain` to connect the retriever to an LLM for answering queries.\n",
    "\n",
    "6. *Chat with PDFs:*\n",
    "    - Implement a RAG-based Q&A system for interacting with PDF documents.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional Enhancements:\n",
    "\n",
    "7. *Semantic Search & Embedding Functions:*\n",
    "    - Perform semantic similarity searches in the VectorStore.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Technology Stack :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*[LangChain](https://python.langchain.com/docs/introduction/)*\n",
    "> framework for developing applications powered by LLMs\n",
    "\n",
    "*[FAISS (Facebook AI Similarity Search)](https://ai.meta.com/tools/faiss/)*\n",
    ">  library allowing storage of contextual embedding vectors in VectorStore and similarity search\n",
    "\n",
    "*[Groq](https://groq.com/about-us/)*\n",
    "> engine providing fast AI inference (conclusion from brand new data) in the cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"../Images/RAG_step1_step2.png\" width=\"800\"/> \n",
    "\n",
    "#### Step 1: Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.vectorstores.faiss import DistanceStrategy\n",
    "from langchain import hub\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdf_data(pdf_path):\n",
    "    \"\"\"\n",
    "    Load text data from PDF file.\n",
    "    \"\"\"\n",
    "    loader = PyPDFLoader(file_path=pdf_path)\n",
    "    documents = loader.load()\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "react_docs = load_pdf_data(pdf_path = \"../documents/react_paper.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show number of pages\n",
    "print(f\"number of loaded pages: {len(react_docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show page content\n",
    "print(react_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Split Documents into Chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why?** \n",
    ">  LLMs have a finite context window.\n",
    "\n",
    ">  Chunking improves retrieval by splitting text into searchable pieces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_documents(documents, chunk_size=200, chunk_overlap=50):\n",
    "    \"\"\"\n",
    "    Splits documents into chunks of given size and overlap\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(documents=documents)\n",
    "    \n",
    "    # Just to add id for etch chunks to map it later \n",
    "    for i, chunk in enumerate(chunks):\n",
    "         chunk.metadata.update({\n",
    "        \"id\": f\"chunk_{i}\",\n",
    "    })\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "\n",
    "react_chunks = split_documents(react_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show number of chunks created\n",
    "print(f\"number of chunks created: {len(react_chunks)}\",\"\\n\",f\"Type of the chunks : {type(react_chunks)}\",\"\\n\\n\" ,react_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Generate and Store Embeddings in a VectorStore\n",
    "<img src=\"../Images/RAG_step3.png\" width=\"400\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why?**\n",
    ">  finding numerical representations of text chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_embedding_vector_db(chunks, db_name):\n",
    "    \"\"\"\n",
    "    This function uses the open-source embedding model HuggingFaceEmbeddings \n",
    "    to create embeddings and store those in a VectorStore called FAISS, \n",
    "    which allows for efficient similarity search\n",
    "    \"\"\"\n",
    "    # instantiate embedding model\n",
    "    embedding = HuggingFaceEmbeddings(\n",
    "        model_name='sentence-transformers/all-mpnet-base-v2',\n",
    "        encode_kwargs={\"normalize_embeddings\": True}\n",
    "    )\n",
    "    # create the vector store \n",
    "    vectorstore = FAISS.from_documents(\n",
    "        documents=chunks,\n",
    "        embedding=embedding,\n",
    "        distance_strategy=DistanceStrategy.COSINE  # or DistanceStrategy.DOT or DistanceStrategy.L2 \n",
    "        \n",
    "    )\n",
    "    # save VectorStore locally\n",
    "    vectorstore.save_local(f\"../vector_databases/vector_db_{db_name}\")\n",
    "    return vectorstore\n",
    "\n",
    "\n",
    "all_embedding=create_embedding_vector_db(chunks=react_chunks, db_name=\"react\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> ‚ö†Ô∏è **Note: Why Embedding Normalization Matters in RAG (with FAISS + Cosine Similarity)**  \n",
    ">  \n",
    "> In Retrieval-Augmented Generation (RAG), accurate retrieval is critical.  \n",
    "> Since most vector search relies on comparing **semantic similarity**,  \n",
    "> we often use **cosine similarity** to find relevant chunks.  \n",
    ">  \n",
    "> However, cosine similarity compares **direction**, not magnitude.  \n",
    "> If your embeddings are **not normalized**, the similarity score can be biased by vector length ‚Äî leading to irrelevant results or lower-quality answers.  \n",
    ">  \n",
    "\n",
    ">  \n",
    "> üîç **Importance of Normalization in FAISS (Cosine Similarity)**  \n",
    ">  \n",
    "> If you set `distance_strategy=\"COSINE\"` but do **not** enable `normalize_embeddings=True`,  \n",
    "> FAISS will default to using **unnormalized dot product** ‚Äî not true cosine similarity.  \n",
    ">  \n",
    "> **As a result:**  \n",
    "> - Similarity scores may be skewed by vector lengths.  \n",
    "> - Retrieval results can become inconsistent or incorrect.  \n",
    ">  \n",
    "> ‚úÖ **Solution:**  \n",
    "> Always normalize embeddings (both **chunks** and **queries**) before storing or searching.  \n",
    "> This ensures the dot product reflects **true cosine similarity**,  \n",
    "> where comparison is based purely on **direction**, not **magnitude**.  \n",
    ">  \n",
    "> ‚ÑπÔ∏è Without normalization, FAISS effectively ignores your `COSINE` strategy and uses inner product directly. [source](https://github.com/langchain-ai/langchain/issues/32498)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../Images/RAG_step4_step5.png\" width=\"600\"/> \n",
    "\n",
    "#### Step 4: Load embedded chunks from VectorStore and make retrieve object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why ?**\n",
    "\n",
    "> make sure that you are using the same embeddings model that you used when you story the chunks\n",
    "\n",
    "> the Chain expecting retriever object as a input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_from_vector_db(vector_db_path):\n",
    "    \"\"\"\n",
    "    this function splits out a retriever object from a local VectorStore\n",
    "    \"\"\"\n",
    "    # instantiate embedding model\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name='sentence-transformers/all-mpnet-base-v2',\n",
    "        encode_kwargs={\"normalize_embeddings\": True}\n",
    "    )\n",
    "    react_vectorstore = FAISS.load_local(\n",
    "        folder_path=vector_db_path,\n",
    "        embeddings=embeddings,\n",
    "        allow_dangerous_deserialization=True,\n",
    "        distance_strategy=DistanceStrategy.COSINE  # or DistanceStrategy.DOT or DistanceStrategy.L2 \n",
    "    )\n",
    "    retriever = react_vectorstore.as_retriever()\n",
    "    return retriever ,react_vectorstore\n",
    "\n",
    "# Load the retriever and index\n",
    "react_retriever,react_vectorstore = retrieve_from_vector_db(\"../vector_databases/vector_db_react\")\n",
    "type(react_retriever),type(react_vectorstore)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "react_retriever.vectorstore.docstore._dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"\"\"\n",
    "what is react ?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "react_retriever.get_relevant_documents(query,k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Connecting the Retriever to LLM and Generate Response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why?**\n",
    "\n",
    "- [`create_stuff_documents_chain`](https://api.python.langchain.com/en/latest/chains/langchain.chains.combine_documents.stuff.create_stuff_documents_chain.html#langchain.chains.combine_documents.stuff.create_stuff_documents_chain) *chain passing documents to llm*\n",
    "  > takes a list of documents and formats them all into a prompt, then passes that prompt to an LLM\n",
    "\n",
    "  >passes ALL documents, so you should make sure it fits within the context window of the LLM being used\n",
    "\n",
    "- [`create_retrieval_chain`](https://api.python.langchain.com/en/latest/chains/langchain.chains.retrieval.create_retrieval_chain.html#langchain.chains.retrieval.create_retrieval_chain) *chain passing user inquiry to retriever object*\n",
    "\n",
    "  > takes in a user inquiry, which is then passed to the retriever to fetch relevant documents\n",
    "  \n",
    "  > those documents (and original inputs) are then passed to an LLM to generate a response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_chains(retriever):\n",
    "    \"\"\"\n",
    "    this function connects stuff_documents_chain with retrieval_chain\n",
    "    \"\"\"\n",
    "    stuff_documents_chain = create_stuff_documents_chain(\n",
    "        llm=llm,\n",
    "        prompt=hub.pull(\"langchain-ai/retrieval-qa-chat\")\n",
    "    )\n",
    "    retrieval_chain = create_retrieval_chain(\n",
    "        retriever=retriever,\n",
    "        combine_docs_chain=stuff_documents_chain\n",
    "    )\n",
    "    return retrieval_chain\n",
    "\n",
    "\n",
    "react_retrieval_chain = connect_chains(react_retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = react_retrieval_chain.invoke(\n",
    "    {\"input\": \"what is zebra?\"}\n",
    ")\n",
    "type(output) , output.keys() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output['context']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6: Chat with PDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paracetamol_docs = load_pdf_data(pdf_path = \"../documents/paracetamol.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split document into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paracetamol_chunks = split_documents(paracetamol_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_embedding_vector_db(chunks=paracetamol_chunks, db_name=\"paracetamol\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Retrieve from VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paracetamol_retriever = retrieve_from_vector_db(\"../vector_databases/vector_db_paracetamol\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paracetamol_retrieval_chain = connect_chains(paracetamol_retriever[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_output(\n",
    "    inquiry,\n",
    "    retrieval_chain=paracetamol_retrieval_chain\n",
    "):\n",
    "    result = retrieval_chain.invoke({\"input\": inquiry})\n",
    "    print(result['answer'].strip(\"\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**inquiry 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_output(\"Give me the summary of Paracetamol in 3 sentences.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**inquiry 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_output(\"Geb mir die Zusammenfassung von Paracetamol in 3 S√§tzen.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 7: Semantic search and embedding function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the create_embedding_vector_db from the Step 3 \n",
    "all_embedding=create_embedding_vector_db(chunks=react_chunks, db_name=\"react\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see all the _dict and the matedata of the chanks\n",
    "all_embedding.docstore._dict.values() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see all the embedding of the chanks\n",
    "all_embedding.index.reconstruct_n() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"5H\\x0c0RFN\\x10XS\\x03,QIRUPDWLRQ\\x03DQG\\x03$SS\"\n",
    "embedding_query = all_embedding.embedding_function.embed_query(query) # you can use all_embedding._embed_query(query) to have the same result\n",
    "len(embedding_query) ,type(embedding_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embedding.similarity_search(query ,k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embedding.similarity_search_with_score(query,k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we should embadding the query becuse the similarity_search_by_vector Expecting embedding input \n",
    "all_embedding.similarity_search_by_vector(embedding_query,k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ensure query_embedding is a NumPy array with float32 type\n",
    "query_embedded_array = np.array([embedding_query], dtype=np.float32)  # Correct variable name\n",
    "\n",
    "# Perform FAISS search\n",
    "distances, indexes = all_embedding.index.search(query_embedded_array, k=3)\n",
    "\n",
    "\n",
    "# Retrieve the embeddings of the top 5 search results\n",
    "retrieved_embeddings = [all_embedding.index.reconstruct(int(idx)) for idx in indexes[0]]\n",
    "\n",
    "# Print embeddings of the retrieved documents\n",
    "print(\"Embeddings of Retrieved Documents:\", retrieved_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid black\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <span style=\"color:orange;\">4. Conclusion</span>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We created an embedding store using FAISS.‚úÖ\n",
    "\n",
    "- Retrieved relevant documents using a retriever object.‚úÖ\n",
    "\n",
    "- Implemented a LangChain pipeline to process and generate responses.‚úÖ\n",
    "\n",
    "- Built an interactive PDF chat system with RAG.‚úÖ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid black\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <span style=\"color:orange;\">5. References</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. [RAG vs. Fine Tuning](https://www.youtube.com/watch?v=00Q0G84kq3M)\n",
    "2. [How to Use Langchain Chain Invoke: A Step-by-Step Guide](https://medium.com/@asakisakamoto02/how-to-use-langchain-chain-invoke-a-step-by-step-guide-9a6f129d77d1)\n",
    "3. [Implementing RAG using Langchain and Ollama](https://medium.com/@imabhi1216/implementing-rag-using-langchain-and-ollama-93bdf4a9027c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
